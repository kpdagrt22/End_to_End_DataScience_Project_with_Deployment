# End-to-End ML Project with MLOps | Modular, Trackable, and Deployable

![AWS](https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg)
![Airflow](https://upload.wikimedia.org/wikipedia/commons/3/3d/AirflowLogo.svg)
![DVC](https://avatars.githubusercontent.com/u/41825161?s=200&v=4)
![MLflow](https://mlflow.org/images/logo.svg)
![Docker](https://upload.wikimedia.org/wikipedia/commons/4/4e/Docker_%28container_engine%29_logo.svg)
![FastAPI](https://seeklogo.com/images/F/fastapi-logo-541F7C3F96-seeklogo.com.png)
![Grafana](https://logowik.com/content/uploads/images/grafana3182.jpg)

## ğŸš€ Project Overview
This project demonstrates a **complete, production-ready data science workflow** implemented using **modular design principles**. It covers every stage of the ML lifecycle â€” from **data ingestion** to **model evaluation and tracking** â€” and adheres to **industry best practices** for configuration, pipeline orchestration, and experiment management.

---

## ğŸ“Œ Features
- **Modular architecture** using Python packages and custom components
- **Version-controlled datasets and models** via DVC and Git
- **ML pipeline orchestration** using Apache Airflow & Astronomer
- **Experiment tracking** with MLflow
- **Deployment-ready app** using FastAPI
- **Monitoring with Grafana dashboards**
- **Config and schema management** with YAML
- **Automated folder and logging setup**

---

## ğŸ§± Project Architecture
```
project-root/
â”‚
â”œâ”€â”€ config/                  # YAML-based config files
â”œâ”€â”€ constants/              # Hard-coded values and constants
â”œâ”€â”€ entity/                 # Data classes for type safety
â”œâ”€â”€ components/             # Core ML pipeline components
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â””â”€â”€ model_evaluation.py
â”‚
â”œâ”€â”€ pipeline/               # Training and prediction pipelines
â”‚
â”œâ”€â”€ utils/                  # Common reusable functions
â”‚
â”œâ”€â”€ logs/                   # Central logging location
â”œâ”€â”€ templates/              # Flask/FastAPI HTML templates (for app)
â”œâ”€â”€ Dockerfile              # Containerization setup
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ setup.py                # For packaging
â”œâ”€â”€ main.py                 # Trigger pipeline execution
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used
- **Languages & Libraries**: Python, Scikit-learn, Pandas, Matplotlib, YAML, Joblib
- **MLOps & Workflow**: MLflow, DVC, Apache Airflow, Astronomer
- **Deployment**: Docker, FastAPI
- **Cloud**: AWS EC2, S3
- **Monitoring**: Grafana
- **Utilities**: ConfigBox, Ensure Annotation, Logging, GitHub Actions

---

## ğŸ§ª ML Lifecycle
1. **Data Ingestion**: Reads zipped data from GitHub, unzips & stores in `artifacts`
2. **Data Validation**: Validates schema using YAML
3. **Data Transformation**: Cleans and prepares features
4. **Model Training**: Trains a RandomForest model, logs with MLflow
5. **Model Evaluation**: Compares metrics, selects best model
6. **Deployment**: Docker + FastAPI endpoint ready for inference
7. **Monitoring**: Metrics tracked using MLflow UI and Grafana dashboards

---

## ğŸ“ Configuration Strategy
All runtime parameters (e.g., file paths, model settings, schema) are defined in:
- `config.yaml`
- `params.yaml`
- `schema.yaml`

These are read using `ConfigBox` for dot-access and type safety.

---

## ğŸ§° How to Run
```bash
# Step 1: Create Conda environment
conda create -p venv python=3.10 -y
conda activate ./venv

# Step 2: Install dependencies
pip install -r requirements.txt

# Step 3: Set up and run project
python template.py     # auto-generates folder structure
python main.py         # runs the training pipeline

# Step 4: Track experiment
mlflow ui              # opens MLflow dashboard

# Optional: Deploy with Docker
docker build -t mlproject .
docker run -p 8080:8080 mlproject
```

---

## ğŸ‘¨â€ğŸ’» Author
**Prakash Kantumutchu**  
AI/ML Engineer | 7.5+ Yrs Experience  
[GitHub](https://github.com/kpdagrt22) | [LinkedIn](https://linkedin.com/in/prakash-kantumutchu)  

---

## ğŸ“Œ Certifications Related to This Project
- AI Agent Developer â€” Vanderbilt University
- Azure AI Engineer Associate (AI-102)
- Machine Learning Scientist with Python â€” DataCamp
- Docker Foundations, GitHub Actions Pro

---

## ğŸŒŸ Highlights
- Designed for real-world, production environments
- Covers both **MLOps** and **modular software engineering**
- Showcases **YAML-based automation**, **logging**, and **versioning**
- Ideal template for interviews, demos, and enterprise workflows

> **"Built with the same mindset used in production teams â€” scalable, testable, and deployable."**

---



*Coming Soon in repo images folder...*

---

## â­ Give a Star
If this repo helped you understand real-world DS pipelines, consider starring it!

